{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEN-kaVNlbCb"
      },
      "outputs": [],
      "source": [
        "# Practical 9: Write a python program in python program for creating a Back Propagation Feed-forward neural network\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrX7nCEelmh2"
      },
      "outputs": [],
      "source": [
        "# Sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(output):\n",
        "    return output * (1 - output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Q43ZMH0l0DT"
      },
      "outputs": [],
      "source": [
        "# Training data (e.g., XOR function or any dataset you want)\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Expected output\n",
        "y = np.array([[0], [1], [1], [0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njDImWNol3Tl"
      },
      "outputs": [],
      "source": [
        "# Seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Network architecture\n",
        "input_neurons = X.shape[1]\n",
        "hidden_neurons = 4\n",
        "output_neurons = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGO5RQ6ml5Rt"
      },
      "outputs": [],
      "source": [
        "# Weight and bias initialization\n",
        "weights_input_hidden = np.random.uniform(size=(input_neurons, hidden_neurons))\n",
        "bias_hidden = np.random.uniform(size=(1, hidden_neurons))\n",
        "weights_hidden_output = np.random.uniform(size=(hidden_neurons, output_neurons))\n",
        "bias_output = np.random.uniform(size=(1, output_neurons))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP6OR-F1l6-3"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "epochs = 1000\n",
        "learning_rate = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-SnVc2yl-GS",
        "outputId": "dc7e5eb2-8ee2-467a-ec55-df8ee31fc719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 0.2497\n",
            "Epoch 1 Loss: 0.2497\n",
            "Epoch 2 Loss: 0.2496\n",
            "Epoch 3 Loss: 0.2496\n",
            "Epoch 4 Loss: 0.2496\n",
            "Epoch 5 Loss: 0.2496\n",
            "Epoch 6 Loss: 0.2496\n",
            "Epoch 7 Loss: 0.2496\n",
            "Epoch 8 Loss: 0.2496\n",
            "Epoch 9 Loss: 0.2496\n",
            "Epoch 10 Loss: 0.2496\n",
            "Epoch 11 Loss: 0.2496\n",
            "Epoch 12 Loss: 0.2496\n",
            "Epoch 13 Loss: 0.2496\n",
            "Epoch 14 Loss: 0.2496\n",
            "Epoch 15 Loss: 0.2496\n",
            "Epoch 16 Loss: 0.2496\n",
            "Epoch 17 Loss: 0.2495\n",
            "Epoch 18 Loss: 0.2495\n",
            "Epoch 19 Loss: 0.2495\n",
            "Epoch 20 Loss: 0.2495\n",
            "Epoch 21 Loss: 0.2495\n",
            "Epoch 22 Loss: 0.2495\n",
            "Epoch 23 Loss: 0.2495\n",
            "Epoch 24 Loss: 0.2495\n",
            "Epoch 25 Loss: 0.2495\n",
            "Epoch 26 Loss: 0.2495\n",
            "Epoch 27 Loss: 0.2495\n",
            "Epoch 28 Loss: 0.2495\n",
            "Epoch 29 Loss: 0.2495\n",
            "Epoch 30 Loss: 0.2495\n",
            "Epoch 31 Loss: 0.2494\n",
            "Epoch 32 Loss: 0.2494\n",
            "Epoch 33 Loss: 0.2494\n",
            "Epoch 34 Loss: 0.2494\n",
            "Epoch 35 Loss: 0.2494\n",
            "Epoch 36 Loss: 0.2494\n",
            "Epoch 37 Loss: 0.2494\n",
            "Epoch 38 Loss: 0.2494\n",
            "Epoch 39 Loss: 0.2494\n",
            "Epoch 40 Loss: 0.2494\n",
            "Epoch 41 Loss: 0.2494\n",
            "Epoch 42 Loss: 0.2494\n",
            "Epoch 43 Loss: 0.2494\n",
            "Epoch 44 Loss: 0.2494\n",
            "Epoch 45 Loss: 0.2493\n",
            "Epoch 46 Loss: 0.2493\n",
            "Epoch 47 Loss: 0.2493\n",
            "Epoch 48 Loss: 0.2493\n",
            "Epoch 49 Loss: 0.2493\n",
            "Epoch 50 Loss: 0.2493\n",
            "Epoch 51 Loss: 0.2493\n",
            "Epoch 52 Loss: 0.2493\n",
            "Epoch 53 Loss: 0.2493\n",
            "Epoch 54 Loss: 0.2493\n",
            "Epoch 55 Loss: 0.2493\n",
            "Epoch 56 Loss: 0.2493\n",
            "Epoch 57 Loss: 0.2492\n",
            "Epoch 58 Loss: 0.2492\n",
            "Epoch 59 Loss: 0.2492\n",
            "Epoch 60 Loss: 0.2492\n",
            "Epoch 61 Loss: 0.2492\n",
            "Epoch 62 Loss: 0.2492\n",
            "Epoch 63 Loss: 0.2492\n",
            "Epoch 64 Loss: 0.2492\n",
            "Epoch 65 Loss: 0.2492\n",
            "Epoch 66 Loss: 0.2492\n",
            "Epoch 67 Loss: 0.2492\n",
            "Epoch 68 Loss: 0.2492\n",
            "Epoch 69 Loss: 0.2491\n",
            "Epoch 70 Loss: 0.2491\n",
            "Epoch 71 Loss: 0.2491\n",
            "Epoch 72 Loss: 0.2491\n",
            "Epoch 73 Loss: 0.2491\n",
            "Epoch 74 Loss: 0.2491\n",
            "Epoch 75 Loss: 0.2491\n",
            "Epoch 76 Loss: 0.2491\n",
            "Epoch 77 Loss: 0.2491\n",
            "Epoch 78 Loss: 0.2491\n",
            "Epoch 79 Loss: 0.2491\n",
            "Epoch 80 Loss: 0.2490\n",
            "Epoch 81 Loss: 0.2490\n",
            "Epoch 82 Loss: 0.2490\n",
            "Epoch 83 Loss: 0.2490\n",
            "Epoch 84 Loss: 0.2490\n",
            "Epoch 85 Loss: 0.2490\n",
            "Epoch 86 Loss: 0.2490\n",
            "Epoch 87 Loss: 0.2490\n",
            "Epoch 88 Loss: 0.2490\n",
            "Epoch 89 Loss: 0.2490\n",
            "Epoch 90 Loss: 0.2489\n",
            "Epoch 91 Loss: 0.2489\n",
            "Epoch 92 Loss: 0.2489\n",
            "Epoch 93 Loss: 0.2489\n",
            "Epoch 94 Loss: 0.2489\n",
            "Epoch 95 Loss: 0.2489\n",
            "Epoch 96 Loss: 0.2489\n",
            "Epoch 97 Loss: 0.2489\n",
            "Epoch 98 Loss: 0.2489\n",
            "Epoch 99 Loss: 0.2489\n",
            "Epoch 100 Loss: 0.2488\n",
            "Epoch 101 Loss: 0.2488\n",
            "Epoch 102 Loss: 0.2488\n",
            "Epoch 103 Loss: 0.2488\n",
            "Epoch 104 Loss: 0.2488\n",
            "Epoch 105 Loss: 0.2488\n",
            "Epoch 106 Loss: 0.2488\n",
            "Epoch 107 Loss: 0.2488\n",
            "Epoch 108 Loss: 0.2488\n",
            "Epoch 109 Loss: 0.2487\n",
            "Epoch 110 Loss: 0.2487\n",
            "Epoch 111 Loss: 0.2487\n",
            "Epoch 112 Loss: 0.2487\n",
            "Epoch 113 Loss: 0.2487\n",
            "Epoch 114 Loss: 0.2487\n",
            "Epoch 115 Loss: 0.2487\n",
            "Epoch 116 Loss: 0.2487\n",
            "Epoch 117 Loss: 0.2487\n",
            "Epoch 118 Loss: 0.2486\n",
            "Epoch 119 Loss: 0.2486\n",
            "Epoch 120 Loss: 0.2486\n",
            "Epoch 121 Loss: 0.2486\n",
            "Epoch 122 Loss: 0.2486\n",
            "Epoch 123 Loss: 0.2486\n",
            "Epoch 124 Loss: 0.2486\n",
            "Epoch 125 Loss: 0.2486\n",
            "Epoch 126 Loss: 0.2485\n",
            "Epoch 127 Loss: 0.2485\n",
            "Epoch 128 Loss: 0.2485\n",
            "Epoch 129 Loss: 0.2485\n",
            "Epoch 130 Loss: 0.2485\n",
            "Epoch 131 Loss: 0.2485\n",
            "Epoch 132 Loss: 0.2485\n",
            "Epoch 133 Loss: 0.2484\n",
            "Epoch 134 Loss: 0.2484\n",
            "Epoch 135 Loss: 0.2484\n",
            "Epoch 136 Loss: 0.2484\n",
            "Epoch 137 Loss: 0.2484\n",
            "Epoch 138 Loss: 0.2484\n",
            "Epoch 139 Loss: 0.2484\n",
            "Epoch 140 Loss: 0.2483\n",
            "Epoch 141 Loss: 0.2483\n",
            "Epoch 142 Loss: 0.2483\n",
            "Epoch 143 Loss: 0.2483\n",
            "Epoch 144 Loss: 0.2483\n",
            "Epoch 145 Loss: 0.2483\n",
            "Epoch 146 Loss: 0.2483\n",
            "Epoch 147 Loss: 0.2482\n",
            "Epoch 148 Loss: 0.2482\n",
            "Epoch 149 Loss: 0.2482\n",
            "Epoch 150 Loss: 0.2482\n",
            "Epoch 151 Loss: 0.2482\n",
            "Epoch 152 Loss: 0.2482\n",
            "Epoch 153 Loss: 0.2482\n",
            "Epoch 154 Loss: 0.2481\n",
            "Epoch 155 Loss: 0.2481\n",
            "Epoch 156 Loss: 0.2481\n",
            "Epoch 157 Loss: 0.2481\n",
            "Epoch 158 Loss: 0.2481\n",
            "Epoch 159 Loss: 0.2481\n",
            "Epoch 160 Loss: 0.2480\n",
            "Epoch 161 Loss: 0.2480\n",
            "Epoch 162 Loss: 0.2480\n",
            "Epoch 163 Loss: 0.2480\n",
            "Epoch 164 Loss: 0.2480\n",
            "Epoch 165 Loss: 0.2480\n",
            "Epoch 166 Loss: 0.2479\n",
            "Epoch 167 Loss: 0.2479\n",
            "Epoch 168 Loss: 0.2479\n",
            "Epoch 169 Loss: 0.2479\n",
            "Epoch 170 Loss: 0.2479\n",
            "Epoch 171 Loss: 0.2478\n",
            "Epoch 172 Loss: 0.2478\n",
            "Epoch 173 Loss: 0.2478\n",
            "Epoch 174 Loss: 0.2478\n",
            "Epoch 175 Loss: 0.2478\n",
            "Epoch 176 Loss: 0.2477\n",
            "Epoch 177 Loss: 0.2477\n",
            "Epoch 178 Loss: 0.2477\n",
            "Epoch 179 Loss: 0.2477\n",
            "Epoch 180 Loss: 0.2477\n",
            "Epoch 181 Loss: 0.2476\n",
            "Epoch 182 Loss: 0.2476\n",
            "Epoch 183 Loss: 0.2476\n",
            "Epoch 184 Loss: 0.2476\n",
            "Epoch 185 Loss: 0.2476\n",
            "Epoch 186 Loss: 0.2475\n",
            "Epoch 187 Loss: 0.2475\n",
            "Epoch 188 Loss: 0.2475\n",
            "Epoch 189 Loss: 0.2475\n",
            "Epoch 190 Loss: 0.2475\n",
            "Epoch 191 Loss: 0.2474\n",
            "Epoch 192 Loss: 0.2474\n",
            "Epoch 193 Loss: 0.2474\n",
            "Epoch 194 Loss: 0.2474\n",
            "Epoch 195 Loss: 0.2473\n",
            "Epoch 196 Loss: 0.2473\n",
            "Epoch 197 Loss: 0.2473\n",
            "Epoch 198 Loss: 0.2473\n",
            "Epoch 199 Loss: 0.2473\n",
            "Epoch 200 Loss: 0.2472\n",
            "Epoch 201 Loss: 0.2472\n",
            "Epoch 202 Loss: 0.2472\n",
            "Epoch 203 Loss: 0.2472\n",
            "Epoch 204 Loss: 0.2471\n",
            "Epoch 205 Loss: 0.2471\n",
            "Epoch 206 Loss: 0.2471\n",
            "Epoch 207 Loss: 0.2471\n",
            "Epoch 208 Loss: 0.2470\n",
            "Epoch 209 Loss: 0.2470\n",
            "Epoch 210 Loss: 0.2470\n",
            "Epoch 211 Loss: 0.2470\n",
            "Epoch 212 Loss: 0.2469\n",
            "Epoch 213 Loss: 0.2469\n",
            "Epoch 214 Loss: 0.2469\n",
            "Epoch 215 Loss: 0.2469\n",
            "Epoch 216 Loss: 0.2468\n",
            "Epoch 217 Loss: 0.2468\n",
            "Epoch 218 Loss: 0.2468\n",
            "Epoch 219 Loss: 0.2467\n",
            "Epoch 220 Loss: 0.2467\n",
            "Epoch 221 Loss: 0.2467\n",
            "Epoch 222 Loss: 0.2467\n",
            "Epoch 223 Loss: 0.2466\n",
            "Epoch 224 Loss: 0.2466\n",
            "Epoch 225 Loss: 0.2466\n",
            "Epoch 226 Loss: 0.2465\n",
            "Epoch 227 Loss: 0.2465\n",
            "Epoch 228 Loss: 0.2465\n",
            "Epoch 229 Loss: 0.2465\n",
            "Epoch 230 Loss: 0.2464\n",
            "Epoch 231 Loss: 0.2464\n",
            "Epoch 232 Loss: 0.2464\n",
            "Epoch 233 Loss: 0.2463\n",
            "Epoch 234 Loss: 0.2463\n",
            "Epoch 235 Loss: 0.2463\n",
            "Epoch 236 Loss: 0.2462\n",
            "Epoch 237 Loss: 0.2462\n",
            "Epoch 238 Loss: 0.2462\n",
            "Epoch 239 Loss: 0.2461\n",
            "Epoch 240 Loss: 0.2461\n",
            "Epoch 241 Loss: 0.2461\n",
            "Epoch 242 Loss: 0.2460\n",
            "Epoch 243 Loss: 0.2460\n",
            "Epoch 244 Loss: 0.2460\n",
            "Epoch 245 Loss: 0.2459\n",
            "Epoch 246 Loss: 0.2459\n",
            "Epoch 247 Loss: 0.2459\n",
            "Epoch 248 Loss: 0.2458\n",
            "Epoch 249 Loss: 0.2458\n",
            "Epoch 250 Loss: 0.2457\n",
            "Epoch 251 Loss: 0.2457\n",
            "Epoch 252 Loss: 0.2457\n",
            "Epoch 253 Loss: 0.2456\n",
            "Epoch 254 Loss: 0.2456\n",
            "Epoch 255 Loss: 0.2456\n",
            "Epoch 256 Loss: 0.2455\n",
            "Epoch 257 Loss: 0.2455\n",
            "Epoch 258 Loss: 0.2454\n",
            "Epoch 259 Loss: 0.2454\n",
            "Epoch 260 Loss: 0.2454\n",
            "Epoch 261 Loss: 0.2453\n",
            "Epoch 262 Loss: 0.2453\n",
            "Epoch 263 Loss: 0.2452\n",
            "Epoch 264 Loss: 0.2452\n",
            "Epoch 265 Loss: 0.2452\n",
            "Epoch 266 Loss: 0.2451\n",
            "Epoch 267 Loss: 0.2451\n",
            "Epoch 268 Loss: 0.2450\n",
            "Epoch 269 Loss: 0.2450\n",
            "Epoch 270 Loss: 0.2449\n",
            "Epoch 271 Loss: 0.2449\n",
            "Epoch 272 Loss: 0.2449\n",
            "Epoch 273 Loss: 0.2448\n",
            "Epoch 274 Loss: 0.2448\n",
            "Epoch 275 Loss: 0.2447\n",
            "Epoch 276 Loss: 0.2447\n",
            "Epoch 277 Loss: 0.2446\n",
            "Epoch 278 Loss: 0.2446\n",
            "Epoch 279 Loss: 0.2445\n",
            "Epoch 280 Loss: 0.2445\n",
            "Epoch 281 Loss: 0.2444\n",
            "Epoch 282 Loss: 0.2444\n",
            "Epoch 283 Loss: 0.2443\n",
            "Epoch 284 Loss: 0.2443\n",
            "Epoch 285 Loss: 0.2442\n",
            "Epoch 286 Loss: 0.2442\n",
            "Epoch 287 Loss: 0.2441\n",
            "Epoch 288 Loss: 0.2441\n",
            "Epoch 289 Loss: 0.2440\n",
            "Epoch 290 Loss: 0.2440\n",
            "Epoch 291 Loss: 0.2439\n",
            "Epoch 292 Loss: 0.2439\n",
            "Epoch 293 Loss: 0.2438\n",
            "Epoch 294 Loss: 0.2438\n",
            "Epoch 295 Loss: 0.2437\n",
            "Epoch 296 Loss: 0.2436\n",
            "Epoch 297 Loss: 0.2436\n",
            "Epoch 298 Loss: 0.2435\n",
            "Epoch 299 Loss: 0.2435\n",
            "Epoch 300 Loss: 0.2434\n",
            "Epoch 301 Loss: 0.2434\n",
            "Epoch 302 Loss: 0.2433\n",
            "Epoch 303 Loss: 0.2432\n",
            "Epoch 304 Loss: 0.2432\n",
            "Epoch 305 Loss: 0.2431\n",
            "Epoch 306 Loss: 0.2431\n",
            "Epoch 307 Loss: 0.2430\n",
            "Epoch 308 Loss: 0.2429\n",
            "Epoch 309 Loss: 0.2429\n",
            "Epoch 310 Loss: 0.2428\n",
            "Epoch 311 Loss: 0.2427\n",
            "Epoch 312 Loss: 0.2427\n",
            "Epoch 313 Loss: 0.2426\n",
            "Epoch 314 Loss: 0.2426\n",
            "Epoch 315 Loss: 0.2425\n",
            "Epoch 316 Loss: 0.2424\n",
            "Epoch 317 Loss: 0.2424\n",
            "Epoch 318 Loss: 0.2423\n",
            "Epoch 319 Loss: 0.2422\n",
            "Epoch 320 Loss: 0.2421\n",
            "Epoch 321 Loss: 0.2421\n",
            "Epoch 322 Loss: 0.2420\n",
            "Epoch 323 Loss: 0.2419\n",
            "Epoch 324 Loss: 0.2419\n",
            "Epoch 325 Loss: 0.2418\n",
            "Epoch 326 Loss: 0.2417\n",
            "Epoch 327 Loss: 0.2416\n",
            "Epoch 328 Loss: 0.2416\n",
            "Epoch 329 Loss: 0.2415\n",
            "Epoch 330 Loss: 0.2414\n",
            "Epoch 331 Loss: 0.2413\n",
            "Epoch 332 Loss: 0.2413\n",
            "Epoch 333 Loss: 0.2412\n",
            "Epoch 334 Loss: 0.2411\n",
            "Epoch 335 Loss: 0.2410\n",
            "Epoch 336 Loss: 0.2409\n",
            "Epoch 337 Loss: 0.2409\n",
            "Epoch 338 Loss: 0.2408\n",
            "Epoch 339 Loss: 0.2407\n",
            "Epoch 340 Loss: 0.2406\n",
            "Epoch 341 Loss: 0.2405\n",
            "Epoch 342 Loss: 0.2405\n",
            "Epoch 343 Loss: 0.2404\n",
            "Epoch 344 Loss: 0.2403\n",
            "Epoch 345 Loss: 0.2402\n",
            "Epoch 346 Loss: 0.2401\n",
            "Epoch 347 Loss: 0.2400\n",
            "Epoch 348 Loss: 0.2399\n",
            "Epoch 349 Loss: 0.2398\n",
            "Epoch 350 Loss: 0.2397\n",
            "Epoch 351 Loss: 0.2397\n",
            "Epoch 352 Loss: 0.2396\n",
            "Epoch 353 Loss: 0.2395\n",
            "Epoch 354 Loss: 0.2394\n",
            "Epoch 355 Loss: 0.2393\n",
            "Epoch 356 Loss: 0.2392\n",
            "Epoch 357 Loss: 0.2391\n",
            "Epoch 358 Loss: 0.2390\n",
            "Epoch 359 Loss: 0.2389\n",
            "Epoch 360 Loss: 0.2388\n",
            "Epoch 361 Loss: 0.2387\n",
            "Epoch 362 Loss: 0.2386\n",
            "Epoch 363 Loss: 0.2385\n",
            "Epoch 364 Loss: 0.2384\n",
            "Epoch 365 Loss: 0.2383\n",
            "Epoch 366 Loss: 0.2382\n",
            "Epoch 367 Loss: 0.2381\n",
            "Epoch 368 Loss: 0.2380\n",
            "Epoch 369 Loss: 0.2379\n",
            "Epoch 370 Loss: 0.2377\n",
            "Epoch 371 Loss: 0.2376\n",
            "Epoch 372 Loss: 0.2375\n",
            "Epoch 373 Loss: 0.2374\n",
            "Epoch 374 Loss: 0.2373\n",
            "Epoch 375 Loss: 0.2372\n",
            "Epoch 376 Loss: 0.2371\n",
            "Epoch 377 Loss: 0.2370\n",
            "Epoch 378 Loss: 0.2368\n",
            "Epoch 379 Loss: 0.2367\n",
            "Epoch 380 Loss: 0.2366\n",
            "Epoch 381 Loss: 0.2365\n",
            "Epoch 382 Loss: 0.2364\n",
            "Epoch 383 Loss: 0.2362\n",
            "Epoch 384 Loss: 0.2361\n",
            "Epoch 385 Loss: 0.2360\n",
            "Epoch 386 Loss: 0.2359\n",
            "Epoch 387 Loss: 0.2357\n",
            "Epoch 388 Loss: 0.2356\n",
            "Epoch 389 Loss: 0.2355\n",
            "Epoch 390 Loss: 0.2353\n",
            "Epoch 391 Loss: 0.2352\n",
            "Epoch 392 Loss: 0.2351\n",
            "Epoch 393 Loss: 0.2349\n",
            "Epoch 394 Loss: 0.2348\n",
            "Epoch 395 Loss: 0.2347\n",
            "Epoch 396 Loss: 0.2345\n",
            "Epoch 397 Loss: 0.2344\n",
            "Epoch 398 Loss: 0.2343\n",
            "Epoch 399 Loss: 0.2341\n",
            "Epoch 400 Loss: 0.2340\n",
            "Epoch 401 Loss: 0.2338\n",
            "Epoch 402 Loss: 0.2337\n",
            "Epoch 403 Loss: 0.2336\n",
            "Epoch 404 Loss: 0.2334\n",
            "Epoch 405 Loss: 0.2333\n",
            "Epoch 406 Loss: 0.2331\n",
            "Epoch 407 Loss: 0.2330\n",
            "Epoch 408 Loss: 0.2328\n",
            "Epoch 409 Loss: 0.2327\n",
            "Epoch 410 Loss: 0.2325\n",
            "Epoch 411 Loss: 0.2323\n",
            "Epoch 412 Loss: 0.2322\n",
            "Epoch 413 Loss: 0.2320\n",
            "Epoch 414 Loss: 0.2319\n",
            "Epoch 415 Loss: 0.2317\n",
            "Epoch 416 Loss: 0.2315\n",
            "Epoch 417 Loss: 0.2314\n",
            "Epoch 418 Loss: 0.2312\n",
            "Epoch 419 Loss: 0.2311\n",
            "Epoch 420 Loss: 0.2309\n",
            "Epoch 421 Loss: 0.2307\n",
            "Epoch 422 Loss: 0.2305\n",
            "Epoch 423 Loss: 0.2304\n",
            "Epoch 424 Loss: 0.2302\n",
            "Epoch 425 Loss: 0.2300\n",
            "Epoch 426 Loss: 0.2299\n",
            "Epoch 427 Loss: 0.2297\n",
            "Epoch 428 Loss: 0.2295\n",
            "Epoch 429 Loss: 0.2293\n",
            "Epoch 430 Loss: 0.2291\n",
            "Epoch 431 Loss: 0.2290\n",
            "Epoch 432 Loss: 0.2288\n",
            "Epoch 433 Loss: 0.2286\n",
            "Epoch 434 Loss: 0.2284\n",
            "Epoch 435 Loss: 0.2282\n",
            "Epoch 436 Loss: 0.2280\n",
            "Epoch 437 Loss: 0.2278\n",
            "Epoch 438 Loss: 0.2276\n",
            "Epoch 439 Loss: 0.2275\n",
            "Epoch 440 Loss: 0.2273\n",
            "Epoch 441 Loss: 0.2271\n",
            "Epoch 442 Loss: 0.2269\n",
            "Epoch 443 Loss: 0.2267\n",
            "Epoch 444 Loss: 0.2265\n",
            "Epoch 445 Loss: 0.2263\n",
            "Epoch 446 Loss: 0.2261\n",
            "Epoch 447 Loss: 0.2259\n",
            "Epoch 448 Loss: 0.2256\n",
            "Epoch 449 Loss: 0.2254\n",
            "Epoch 450 Loss: 0.2252\n",
            "Epoch 451 Loss: 0.2250\n",
            "Epoch 452 Loss: 0.2248\n",
            "Epoch 453 Loss: 0.2246\n",
            "Epoch 454 Loss: 0.2244\n",
            "Epoch 455 Loss: 0.2242\n",
            "Epoch 456 Loss: 0.2239\n",
            "Epoch 457 Loss: 0.2237\n",
            "Epoch 458 Loss: 0.2235\n",
            "Epoch 459 Loss: 0.2233\n",
            "Epoch 460 Loss: 0.2231\n",
            "Epoch 461 Loss: 0.2228\n",
            "Epoch 462 Loss: 0.2226\n",
            "Epoch 463 Loss: 0.2224\n",
            "Epoch 464 Loss: 0.2222\n",
            "Epoch 465 Loss: 0.2219\n",
            "Epoch 466 Loss: 0.2217\n",
            "Epoch 467 Loss: 0.2215\n",
            "Epoch 468 Loss: 0.2212\n",
            "Epoch 469 Loss: 0.2210\n",
            "Epoch 470 Loss: 0.2208\n",
            "Epoch 471 Loss: 0.2205\n",
            "Epoch 472 Loss: 0.2203\n",
            "Epoch 473 Loss: 0.2200\n",
            "Epoch 474 Loss: 0.2198\n",
            "Epoch 475 Loss: 0.2196\n",
            "Epoch 476 Loss: 0.2193\n",
            "Epoch 477 Loss: 0.2191\n",
            "Epoch 478 Loss: 0.2188\n",
            "Epoch 479 Loss: 0.2186\n",
            "Epoch 480 Loss: 0.2183\n",
            "Epoch 481 Loss: 0.2181\n",
            "Epoch 482 Loss: 0.2178\n",
            "Epoch 483 Loss: 0.2176\n",
            "Epoch 484 Loss: 0.2173\n",
            "Epoch 485 Loss: 0.2170\n",
            "Epoch 486 Loss: 0.2168\n",
            "Epoch 487 Loss: 0.2165\n",
            "Epoch 488 Loss: 0.2163\n",
            "Epoch 489 Loss: 0.2160\n",
            "Epoch 490 Loss: 0.2158\n",
            "Epoch 491 Loss: 0.2155\n",
            "Epoch 492 Loss: 0.2152\n",
            "Epoch 493 Loss: 0.2150\n",
            "Epoch 494 Loss: 0.2147\n",
            "Epoch 495 Loss: 0.2144\n",
            "Epoch 496 Loss: 0.2142\n",
            "Epoch 497 Loss: 0.2139\n",
            "Epoch 498 Loss: 0.2136\n",
            "Epoch 499 Loss: 0.2133\n",
            "Epoch 500 Loss: 0.2131\n",
            "Epoch 501 Loss: 0.2128\n",
            "Epoch 502 Loss: 0.2125\n",
            "Epoch 503 Loss: 0.2122\n",
            "Epoch 504 Loss: 0.2120\n",
            "Epoch 505 Loss: 0.2117\n",
            "Epoch 506 Loss: 0.2114\n",
            "Epoch 507 Loss: 0.2111\n",
            "Epoch 508 Loss: 0.2109\n",
            "Epoch 509 Loss: 0.2106\n",
            "Epoch 510 Loss: 0.2103\n",
            "Epoch 511 Loss: 0.2100\n",
            "Epoch 512 Loss: 0.2097\n",
            "Epoch 513 Loss: 0.2094\n",
            "Epoch 514 Loss: 0.2092\n",
            "Epoch 515 Loss: 0.2089\n",
            "Epoch 516 Loss: 0.2086\n",
            "Epoch 517 Loss: 0.2083\n",
            "Epoch 518 Loss: 0.2080\n",
            "Epoch 519 Loss: 0.2077\n",
            "Epoch 520 Loss: 0.2074\n",
            "Epoch 521 Loss: 0.2071\n",
            "Epoch 522 Loss: 0.2068\n",
            "Epoch 523 Loss: 0.2065\n",
            "Epoch 524 Loss: 0.2063\n",
            "Epoch 525 Loss: 0.2060\n",
            "Epoch 526 Loss: 0.2057\n",
            "Epoch 527 Loss: 0.2054\n",
            "Epoch 528 Loss: 0.2051\n",
            "Epoch 529 Loss: 0.2048\n",
            "Epoch 530 Loss: 0.2045\n",
            "Epoch 531 Loss: 0.2042\n",
            "Epoch 532 Loss: 0.2039\n",
            "Epoch 533 Loss: 0.2036\n",
            "Epoch 534 Loss: 0.2033\n",
            "Epoch 535 Loss: 0.2030\n",
            "Epoch 536 Loss: 0.2027\n",
            "Epoch 537 Loss: 0.2024\n",
            "Epoch 538 Loss: 0.2021\n",
            "Epoch 539 Loss: 0.2018\n",
            "Epoch 540 Loss: 0.2015\n",
            "Epoch 541 Loss: 0.2012\n",
            "Epoch 542 Loss: 0.2009\n",
            "Epoch 543 Loss: 0.2006\n",
            "Epoch 544 Loss: 0.2003\n",
            "Epoch 545 Loss: 0.2000\n",
            "Epoch 546 Loss: 0.1997\n",
            "Epoch 547 Loss: 0.1994\n",
            "Epoch 548 Loss: 0.1991\n",
            "Epoch 549 Loss: 0.1988\n",
            "Epoch 550 Loss: 0.1985\n",
            "Epoch 551 Loss: 0.1982\n",
            "Epoch 552 Loss: 0.1979\n",
            "Epoch 553 Loss: 0.1976\n",
            "Epoch 554 Loss: 0.1973\n",
            "Epoch 555 Loss: 0.1969\n",
            "Epoch 556 Loss: 0.1966\n",
            "Epoch 557 Loss: 0.1963\n",
            "Epoch 558 Loss: 0.1960\n",
            "Epoch 559 Loss: 0.1957\n",
            "Epoch 560 Loss: 0.1954\n",
            "Epoch 561 Loss: 0.1951\n",
            "Epoch 562 Loss: 0.1948\n",
            "Epoch 563 Loss: 0.1945\n",
            "Epoch 564 Loss: 0.1942\n",
            "Epoch 565 Loss: 0.1939\n",
            "Epoch 566 Loss: 0.1936\n",
            "Epoch 567 Loss: 0.1933\n",
            "Epoch 568 Loss: 0.1930\n",
            "Epoch 569 Loss: 0.1927\n",
            "Epoch 570 Loss: 0.1924\n",
            "Epoch 571 Loss: 0.1920\n",
            "Epoch 572 Loss: 0.1917\n",
            "Epoch 573 Loss: 0.1914\n",
            "Epoch 574 Loss: 0.1911\n",
            "Epoch 575 Loss: 0.1908\n",
            "Epoch 576 Loss: 0.1905\n",
            "Epoch 577 Loss: 0.1902\n",
            "Epoch 578 Loss: 0.1899\n",
            "Epoch 579 Loss: 0.1896\n",
            "Epoch 580 Loss: 0.1893\n",
            "Epoch 581 Loss: 0.1890\n",
            "Epoch 582 Loss: 0.1887\n",
            "Epoch 583 Loss: 0.1884\n",
            "Epoch 584 Loss: 0.1881\n",
            "Epoch 585 Loss: 0.1877\n",
            "Epoch 586 Loss: 0.1874\n",
            "Epoch 587 Loss: 0.1871\n",
            "Epoch 588 Loss: 0.1868\n",
            "Epoch 589 Loss: 0.1865\n",
            "Epoch 590 Loss: 0.1862\n",
            "Epoch 591 Loss: 0.1859\n",
            "Epoch 592 Loss: 0.1856\n",
            "Epoch 593 Loss: 0.1853\n",
            "Epoch 594 Loss: 0.1850\n",
            "Epoch 595 Loss: 0.1847\n",
            "Epoch 596 Loss: 0.1844\n",
            "Epoch 597 Loss: 0.1841\n",
            "Epoch 598 Loss: 0.1838\n",
            "Epoch 599 Loss: 0.1834\n",
            "Epoch 600 Loss: 0.1831\n",
            "Epoch 601 Loss: 0.1828\n",
            "Epoch 602 Loss: 0.1825\n",
            "Epoch 603 Loss: 0.1822\n",
            "Epoch 604 Loss: 0.1819\n",
            "Epoch 605 Loss: 0.1816\n",
            "Epoch 606 Loss: 0.1813\n",
            "Epoch 607 Loss: 0.1810\n",
            "Epoch 608 Loss: 0.1807\n",
            "Epoch 609 Loss: 0.1804\n",
            "Epoch 610 Loss: 0.1801\n",
            "Epoch 611 Loss: 0.1798\n",
            "Epoch 612 Loss: 0.1794\n",
            "Epoch 613 Loss: 0.1791\n",
            "Epoch 614 Loss: 0.1788\n",
            "Epoch 615 Loss: 0.1785\n",
            "Epoch 616 Loss: 0.1782\n",
            "Epoch 617 Loss: 0.1779\n",
            "Epoch 618 Loss: 0.1776\n",
            "Epoch 619 Loss: 0.1773\n",
            "Epoch 620 Loss: 0.1770\n",
            "Epoch 621 Loss: 0.1767\n",
            "Epoch 622 Loss: 0.1763\n",
            "Epoch 623 Loss: 0.1760\n",
            "Epoch 624 Loss: 0.1757\n",
            "Epoch 625 Loss: 0.1754\n",
            "Epoch 626 Loss: 0.1751\n",
            "Epoch 627 Loss: 0.1748\n",
            "Epoch 628 Loss: 0.1745\n",
            "Epoch 629 Loss: 0.1742\n",
            "Epoch 630 Loss: 0.1738\n",
            "Epoch 631 Loss: 0.1735\n",
            "Epoch 632 Loss: 0.1732\n",
            "Epoch 633 Loss: 0.1729\n",
            "Epoch 634 Loss: 0.1726\n",
            "Epoch 635 Loss: 0.1723\n",
            "Epoch 636 Loss: 0.1719\n",
            "Epoch 637 Loss: 0.1716\n",
            "Epoch 638 Loss: 0.1713\n",
            "Epoch 639 Loss: 0.1710\n",
            "Epoch 640 Loss: 0.1707\n",
            "Epoch 641 Loss: 0.1703\n",
            "Epoch 642 Loss: 0.1700\n",
            "Epoch 643 Loss: 0.1697\n",
            "Epoch 644 Loss: 0.1694\n",
            "Epoch 645 Loss: 0.1690\n",
            "Epoch 646 Loss: 0.1687\n",
            "Epoch 647 Loss: 0.1684\n",
            "Epoch 648 Loss: 0.1681\n",
            "Epoch 649 Loss: 0.1677\n",
            "Epoch 650 Loss: 0.1674\n",
            "Epoch 651 Loss: 0.1671\n",
            "Epoch 652 Loss: 0.1667\n",
            "Epoch 653 Loss: 0.1664\n",
            "Epoch 654 Loss: 0.1661\n",
            "Epoch 655 Loss: 0.1657\n",
            "Epoch 656 Loss: 0.1654\n",
            "Epoch 657 Loss: 0.1650\n",
            "Epoch 658 Loss: 0.1647\n",
            "Epoch 659 Loss: 0.1644\n",
            "Epoch 660 Loss: 0.1640\n",
            "Epoch 661 Loss: 0.1637\n",
            "Epoch 662 Loss: 0.1633\n",
            "Epoch 663 Loss: 0.1630\n",
            "Epoch 664 Loss: 0.1626\n",
            "Epoch 665 Loss: 0.1623\n",
            "Epoch 666 Loss: 0.1619\n",
            "Epoch 667 Loss: 0.1615\n",
            "Epoch 668 Loss: 0.1612\n",
            "Epoch 669 Loss: 0.1608\n",
            "Epoch 670 Loss: 0.1605\n",
            "Epoch 671 Loss: 0.1601\n",
            "Epoch 672 Loss: 0.1597\n",
            "Epoch 673 Loss: 0.1594\n",
            "Epoch 674 Loss: 0.1590\n",
            "Epoch 675 Loss: 0.1586\n",
            "Epoch 676 Loss: 0.1582\n",
            "Epoch 677 Loss: 0.1579\n",
            "Epoch 678 Loss: 0.1575\n",
            "Epoch 679 Loss: 0.1571\n",
            "Epoch 680 Loss: 0.1567\n",
            "Epoch 681 Loss: 0.1563\n",
            "Epoch 682 Loss: 0.1559\n",
            "Epoch 683 Loss: 0.1555\n",
            "Epoch 684 Loss: 0.1551\n",
            "Epoch 685 Loss: 0.1547\n",
            "Epoch 686 Loss: 0.1543\n",
            "Epoch 687 Loss: 0.1539\n",
            "Epoch 688 Loss: 0.1535\n",
            "Epoch 689 Loss: 0.1531\n",
            "Epoch 690 Loss: 0.1527\n",
            "Epoch 691 Loss: 0.1523\n",
            "Epoch 692 Loss: 0.1518\n",
            "Epoch 693 Loss: 0.1514\n",
            "Epoch 694 Loss: 0.1510\n",
            "Epoch 695 Loss: 0.1505\n",
            "Epoch 696 Loss: 0.1501\n",
            "Epoch 697 Loss: 0.1497\n",
            "Epoch 698 Loss: 0.1492\n",
            "Epoch 699 Loss: 0.1488\n",
            "Epoch 700 Loss: 0.1483\n",
            "Epoch 701 Loss: 0.1479\n",
            "Epoch 702 Loss: 0.1474\n",
            "Epoch 703 Loss: 0.1470\n",
            "Epoch 704 Loss: 0.1465\n",
            "Epoch 705 Loss: 0.1460\n",
            "Epoch 706 Loss: 0.1456\n",
            "Epoch 707 Loss: 0.1451\n",
            "Epoch 708 Loss: 0.1446\n",
            "Epoch 709 Loss: 0.1442\n",
            "Epoch 710 Loss: 0.1437\n",
            "Epoch 711 Loss: 0.1432\n",
            "Epoch 712 Loss: 0.1427\n",
            "Epoch 713 Loss: 0.1422\n",
            "Epoch 714 Loss: 0.1417\n",
            "Epoch 715 Loss: 0.1412\n",
            "Epoch 716 Loss: 0.1407\n",
            "Epoch 717 Loss: 0.1402\n",
            "Epoch 718 Loss: 0.1397\n",
            "Epoch 719 Loss: 0.1392\n",
            "Epoch 720 Loss: 0.1386\n",
            "Epoch 721 Loss: 0.1381\n",
            "Epoch 722 Loss: 0.1376\n",
            "Epoch 723 Loss: 0.1371\n",
            "Epoch 724 Loss: 0.1365\n",
            "Epoch 725 Loss: 0.1360\n",
            "Epoch 726 Loss: 0.1355\n",
            "Epoch 727 Loss: 0.1349\n",
            "Epoch 728 Loss: 0.1344\n",
            "Epoch 729 Loss: 0.1338\n",
            "Epoch 730 Loss: 0.1333\n",
            "Epoch 731 Loss: 0.1327\n",
            "Epoch 732 Loss: 0.1322\n",
            "Epoch 733 Loss: 0.1316\n",
            "Epoch 734 Loss: 0.1311\n",
            "Epoch 735 Loss: 0.1305\n",
            "Epoch 736 Loss: 0.1299\n",
            "Epoch 737 Loss: 0.1294\n",
            "Epoch 738 Loss: 0.1288\n",
            "Epoch 739 Loss: 0.1282\n",
            "Epoch 740 Loss: 0.1277\n",
            "Epoch 741 Loss: 0.1271\n",
            "Epoch 742 Loss: 0.1265\n",
            "Epoch 743 Loss: 0.1259\n",
            "Epoch 744 Loss: 0.1253\n",
            "Epoch 745 Loss: 0.1247\n",
            "Epoch 746 Loss: 0.1242\n",
            "Epoch 747 Loss: 0.1236\n",
            "Epoch 748 Loss: 0.1230\n",
            "Epoch 749 Loss: 0.1224\n",
            "Epoch 750 Loss: 0.1218\n",
            "Epoch 751 Loss: 0.1212\n",
            "Epoch 752 Loss: 0.1206\n",
            "Epoch 753 Loss: 0.1200\n",
            "Epoch 754 Loss: 0.1194\n",
            "Epoch 755 Loss: 0.1188\n",
            "Epoch 756 Loss: 0.1182\n",
            "Epoch 757 Loss: 0.1176\n",
            "Epoch 758 Loss: 0.1170\n",
            "Epoch 759 Loss: 0.1164\n",
            "Epoch 760 Loss: 0.1158\n",
            "Epoch 761 Loss: 0.1152\n",
            "Epoch 762 Loss: 0.1146\n",
            "Epoch 763 Loss: 0.1140\n",
            "Epoch 764 Loss: 0.1134\n",
            "Epoch 765 Loss: 0.1128\n",
            "Epoch 766 Loss: 0.1122\n",
            "Epoch 767 Loss: 0.1116\n",
            "Epoch 768 Loss: 0.1110\n",
            "Epoch 769 Loss: 0.1104\n",
            "Epoch 770 Loss: 0.1098\n",
            "Epoch 771 Loss: 0.1092\n",
            "Epoch 772 Loss: 0.1086\n",
            "Epoch 773 Loss: 0.1080\n",
            "Epoch 774 Loss: 0.1074\n",
            "Epoch 775 Loss: 0.1068\n",
            "Epoch 776 Loss: 0.1062\n",
            "Epoch 777 Loss: 0.1056\n",
            "Epoch 778 Loss: 0.1050\n",
            "Epoch 779 Loss: 0.1044\n",
            "Epoch 780 Loss: 0.1038\n",
            "Epoch 781 Loss: 0.1032\n",
            "Epoch 782 Loss: 0.1026\n",
            "Epoch 783 Loss: 0.1020\n",
            "Epoch 784 Loss: 0.1014\n",
            "Epoch 785 Loss: 0.1008\n",
            "Epoch 786 Loss: 0.1003\n",
            "Epoch 787 Loss: 0.0997\n",
            "Epoch 788 Loss: 0.0991\n",
            "Epoch 789 Loss: 0.0985\n",
            "Epoch 790 Loss: 0.0979\n",
            "Epoch 791 Loss: 0.0973\n",
            "Epoch 792 Loss: 0.0968\n",
            "Epoch 793 Loss: 0.0962\n",
            "Epoch 794 Loss: 0.0956\n",
            "Epoch 795 Loss: 0.0951\n",
            "Epoch 796 Loss: 0.0945\n",
            "Epoch 797 Loss: 0.0939\n",
            "Epoch 798 Loss: 0.0934\n",
            "Epoch 799 Loss: 0.0928\n",
            "Epoch 800 Loss: 0.0922\n",
            "Epoch 801 Loss: 0.0917\n",
            "Epoch 802 Loss: 0.0911\n",
            "Epoch 803 Loss: 0.0906\n",
            "Epoch 804 Loss: 0.0900\n",
            "Epoch 805 Loss: 0.0895\n",
            "Epoch 806 Loss: 0.0889\n",
            "Epoch 807 Loss: 0.0884\n",
            "Epoch 808 Loss: 0.0879\n",
            "Epoch 809 Loss: 0.0873\n",
            "Epoch 810 Loss: 0.0868\n",
            "Epoch 811 Loss: 0.0863\n",
            "Epoch 812 Loss: 0.0857\n",
            "Epoch 813 Loss: 0.0852\n",
            "Epoch 814 Loss: 0.0847\n",
            "Epoch 815 Loss: 0.0842\n",
            "Epoch 816 Loss: 0.0836\n",
            "Epoch 817 Loss: 0.0831\n",
            "Epoch 818 Loss: 0.0826\n",
            "Epoch 819 Loss: 0.0821\n",
            "Epoch 820 Loss: 0.0816\n",
            "Epoch 821 Loss: 0.0811\n",
            "Epoch 822 Loss: 0.0806\n",
            "Epoch 823 Loss: 0.0801\n",
            "Epoch 824 Loss: 0.0796\n",
            "Epoch 825 Loss: 0.0791\n",
            "Epoch 826 Loss: 0.0786\n",
            "Epoch 827 Loss: 0.0781\n",
            "Epoch 828 Loss: 0.0777\n",
            "Epoch 829 Loss: 0.0772\n",
            "Epoch 830 Loss: 0.0767\n",
            "Epoch 831 Loss: 0.0762\n",
            "Epoch 832 Loss: 0.0758\n",
            "Epoch 833 Loss: 0.0753\n",
            "Epoch 834 Loss: 0.0748\n",
            "Epoch 835 Loss: 0.0744\n",
            "Epoch 836 Loss: 0.0739\n",
            "Epoch 837 Loss: 0.0734\n",
            "Epoch 838 Loss: 0.0730\n",
            "Epoch 839 Loss: 0.0725\n",
            "Epoch 840 Loss: 0.0721\n",
            "Epoch 841 Loss: 0.0716\n",
            "Epoch 842 Loss: 0.0712\n",
            "Epoch 843 Loss: 0.0708\n",
            "Epoch 844 Loss: 0.0703\n",
            "Epoch 845 Loss: 0.0699\n",
            "Epoch 846 Loss: 0.0695\n",
            "Epoch 847 Loss: 0.0690\n",
            "Epoch 848 Loss: 0.0686\n",
            "Epoch 849 Loss: 0.0682\n",
            "Epoch 850 Loss: 0.0678\n",
            "Epoch 851 Loss: 0.0674\n",
            "Epoch 852 Loss: 0.0669\n",
            "Epoch 853 Loss: 0.0665\n",
            "Epoch 854 Loss: 0.0661\n",
            "Epoch 855 Loss: 0.0657\n",
            "Epoch 856 Loss: 0.0653\n",
            "Epoch 857 Loss: 0.0649\n",
            "Epoch 858 Loss: 0.0645\n",
            "Epoch 859 Loss: 0.0641\n",
            "Epoch 860 Loss: 0.0637\n",
            "Epoch 861 Loss: 0.0633\n",
            "Epoch 862 Loss: 0.0630\n",
            "Epoch 863 Loss: 0.0626\n",
            "Epoch 864 Loss: 0.0622\n",
            "Epoch 865 Loss: 0.0618\n",
            "Epoch 866 Loss: 0.0614\n",
            "Epoch 867 Loss: 0.0611\n",
            "Epoch 868 Loss: 0.0607\n",
            "Epoch 869 Loss: 0.0603\n",
            "Epoch 870 Loss: 0.0600\n",
            "Epoch 871 Loss: 0.0596\n",
            "Epoch 872 Loss: 0.0593\n",
            "Epoch 873 Loss: 0.0589\n",
            "Epoch 874 Loss: 0.0585\n",
            "Epoch 875 Loss: 0.0582\n",
            "Epoch 876 Loss: 0.0578\n",
            "Epoch 877 Loss: 0.0575\n",
            "Epoch 878 Loss: 0.0572\n",
            "Epoch 879 Loss: 0.0568\n",
            "Epoch 880 Loss: 0.0565\n",
            "Epoch 881 Loss: 0.0561\n",
            "Epoch 882 Loss: 0.0558\n",
            "Epoch 883 Loss: 0.0555\n",
            "Epoch 884 Loss: 0.0552\n",
            "Epoch 885 Loss: 0.0548\n",
            "Epoch 886 Loss: 0.0545\n",
            "Epoch 887 Loss: 0.0542\n",
            "Epoch 888 Loss: 0.0539\n",
            "Epoch 889 Loss: 0.0535\n",
            "Epoch 890 Loss: 0.0532\n",
            "Epoch 891 Loss: 0.0529\n",
            "Epoch 892 Loss: 0.0526\n",
            "Epoch 893 Loss: 0.0523\n",
            "Epoch 894 Loss: 0.0520\n",
            "Epoch 895 Loss: 0.0517\n",
            "Epoch 896 Loss: 0.0514\n",
            "Epoch 897 Loss: 0.0511\n",
            "Epoch 898 Loss: 0.0508\n",
            "Epoch 899 Loss: 0.0505\n",
            "Epoch 900 Loss: 0.0502\n",
            "Epoch 901 Loss: 0.0499\n",
            "Epoch 902 Loss: 0.0496\n",
            "Epoch 903 Loss: 0.0494\n",
            "Epoch 904 Loss: 0.0491\n",
            "Epoch 905 Loss: 0.0488\n",
            "Epoch 906 Loss: 0.0485\n",
            "Epoch 907 Loss: 0.0482\n",
            "Epoch 908 Loss: 0.0480\n",
            "Epoch 909 Loss: 0.0477\n",
            "Epoch 910 Loss: 0.0474\n",
            "Epoch 911 Loss: 0.0472\n",
            "Epoch 912 Loss: 0.0469\n",
            "Epoch 913 Loss: 0.0466\n",
            "Epoch 914 Loss: 0.0464\n",
            "Epoch 915 Loss: 0.0461\n",
            "Epoch 916 Loss: 0.0458\n",
            "Epoch 917 Loss: 0.0456\n",
            "Epoch 918 Loss: 0.0453\n",
            "Epoch 919 Loss: 0.0451\n",
            "Epoch 920 Loss: 0.0448\n",
            "Epoch 921 Loss: 0.0446\n",
            "Epoch 922 Loss: 0.0443\n",
            "Epoch 923 Loss: 0.0441\n",
            "Epoch 924 Loss: 0.0438\n",
            "Epoch 925 Loss: 0.0436\n",
            "Epoch 926 Loss: 0.0434\n",
            "Epoch 927 Loss: 0.0431\n",
            "Epoch 928 Loss: 0.0429\n",
            "Epoch 929 Loss: 0.0426\n",
            "Epoch 930 Loss: 0.0424\n",
            "Epoch 931 Loss: 0.0422\n",
            "Epoch 932 Loss: 0.0420\n",
            "Epoch 933 Loss: 0.0417\n",
            "Epoch 934 Loss: 0.0415\n",
            "Epoch 935 Loss: 0.0413\n",
            "Epoch 936 Loss: 0.0411\n",
            "Epoch 937 Loss: 0.0408\n",
            "Epoch 938 Loss: 0.0406\n",
            "Epoch 939 Loss: 0.0404\n",
            "Epoch 940 Loss: 0.0402\n",
            "Epoch 941 Loss: 0.0400\n",
            "Epoch 942 Loss: 0.0398\n",
            "Epoch 943 Loss: 0.0395\n",
            "Epoch 944 Loss: 0.0393\n",
            "Epoch 945 Loss: 0.0391\n",
            "Epoch 946 Loss: 0.0389\n",
            "Epoch 947 Loss: 0.0387\n",
            "Epoch 948 Loss: 0.0385\n",
            "Epoch 949 Loss: 0.0383\n",
            "Epoch 950 Loss: 0.0381\n",
            "Epoch 951 Loss: 0.0379\n",
            "Epoch 952 Loss: 0.0377\n",
            "Epoch 953 Loss: 0.0375\n",
            "Epoch 954 Loss: 0.0373\n",
            "Epoch 955 Loss: 0.0371\n",
            "Epoch 956 Loss: 0.0369\n",
            "Epoch 957 Loss: 0.0367\n",
            "Epoch 958 Loss: 0.0365\n",
            "Epoch 959 Loss: 0.0364\n",
            "Epoch 960 Loss: 0.0362\n",
            "Epoch 961 Loss: 0.0360\n",
            "Epoch 962 Loss: 0.0358\n",
            "Epoch 963 Loss: 0.0356\n",
            "Epoch 964 Loss: 0.0354\n",
            "Epoch 965 Loss: 0.0353\n",
            "Epoch 966 Loss: 0.0351\n",
            "Epoch 967 Loss: 0.0349\n",
            "Epoch 968 Loss: 0.0347\n",
            "Epoch 969 Loss: 0.0346\n",
            "Epoch 970 Loss: 0.0344\n",
            "Epoch 971 Loss: 0.0342\n",
            "Epoch 972 Loss: 0.0340\n",
            "Epoch 973 Loss: 0.0339\n",
            "Epoch 974 Loss: 0.0337\n",
            "Epoch 975 Loss: 0.0335\n",
            "Epoch 976 Loss: 0.0334\n",
            "Epoch 977 Loss: 0.0332\n",
            "Epoch 978 Loss: 0.0330\n",
            "Epoch 979 Loss: 0.0329\n",
            "Epoch 980 Loss: 0.0327\n",
            "Epoch 981 Loss: 0.0325\n",
            "Epoch 982 Loss: 0.0324\n",
            "Epoch 983 Loss: 0.0322\n",
            "Epoch 984 Loss: 0.0321\n",
            "Epoch 985 Loss: 0.0319\n",
            "Epoch 986 Loss: 0.0318\n",
            "Epoch 987 Loss: 0.0316\n",
            "Epoch 988 Loss: 0.0315\n",
            "Epoch 989 Loss: 0.0313\n",
            "Epoch 990 Loss: 0.0312\n",
            "Epoch 991 Loss: 0.0310\n",
            "Epoch 992 Loss: 0.0309\n",
            "Epoch 993 Loss: 0.0307\n",
            "Epoch 994 Loss: 0.0306\n",
            "Epoch 995 Loss: 0.0304\n",
            "Epoch 996 Loss: 0.0303\n",
            "Epoch 997 Loss: 0.0301\n",
            "Epoch 998 Loss: 0.0300\n",
            "Epoch 999 Loss: 0.0298\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # --- Forward Propagation ---\n",
        "    hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
        "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "\n",
        "    final_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
        "    predicted_output = sigmoid(final_input)\n",
        "\n",
        "    # --- Backward Propagation ---\n",
        "    error = y - predicted_output\n",
        "    d_output = error * sigmoid_derivative(predicted_output)\n",
        "\n",
        "    error_hidden = d_output.dot(weights_hidden_output.T)\n",
        "    d_hidden = error_hidden * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "    # Update weights and biases\n",
        "    weights_hidden_output += hidden_layer_output.T.dot(d_output) * learning_rate\n",
        "    bias_output += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    weights_input_hidden += X.T.dot(d_hidden) * learning_rate\n",
        "    bias_hidden += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    # Optionally print loss\n",
        "    if epoch % 1 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch} Loss: {loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGIjz09hmC37",
        "outputId": "51676577-bd9d-48f5-d7c6-fdc0e4aa1f66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final output after training:\n",
            "[[0.118]\n",
            " [0.853]\n",
            " [0.804]\n",
            " [0.213]]\n"
          ]
        }
      ],
      "source": [
        "# Final output\n",
        "print(\"\\nFinal output after training:\")\n",
        "print(np.round(predicted_output, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zcYjc-BmGUa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
