{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZCduKn9L9SLf"
      },
      "outputs": [],
      "source": [
        "#part1\n",
        "sample_doc = \"Natural Language Processing (NLP) is a sub-field of artificial intelligence concerned with understanding and processing human language.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO7sTsGk9UIh",
        "outputId": "91a64152-86c3-4b26-efba-4c09f96ec65c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenise\n",
        "tokens = word_tokenize(sample_doc)\n",
        "print(\"Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFj6OBh-9ZNd",
        "outputId": "3bc371ac-cc01-4022-bb96-ac12b297c808"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'sub-field', 'of', 'artificial', 'intelligence', 'concerned', 'with', 'understanding', 'and', 'processing', 'human', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pos tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(\"POS Tags:\", pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmhIFN-x9fkt",
        "outputId": "f15cd024-ae30-465b-b754-a946fdbe97cf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('sub-field', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('concerned', 'VBN'), ('with', 'IN'), ('understanding', 'JJ'), ('and', 'CC'), ('processing', 'JJ'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stop-words removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
        "print(\"Filtered Tokens:\", filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tgle_rPU9vLD",
        "outputId": "9e481c16-4ee5-4ebc-a799-8d9b6b6773b6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens: ['Natural', 'Language', 'Processing', 'NLP', 'artificial', 'intelligence', 'concerned', 'understanding', 'processing', 'human', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEm5CGRy99Bn",
        "outputId": "e238b806-d1e2-4b09-8de9-4728d5abd342"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Tokens: ['natur', 'languag', 'process', 'nlp', 'artifici', 'intellig', 'concern', 'understand', 'process', 'human', 'languag']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatizing\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak9h4dcb-AJj",
        "outputId": "5eabf59d-4f7f-4250-fd50-13de5a125d10"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Tokens: ['Natural', 'Language', 'Processing', 'NLP', 'artificial', 'intelligence', 'concerned', 'understanding', 'processing', 'human', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#part2\n",
        "#TF-IDF Representation\n",
        "documents = [\n",
        "    \"Natural Language Processing helps computers understand human language.\",\n",
        "    \"Artificial Intelligence includes machine learning and NLP.\",\n",
        "    \"Text preprocessing includes tokenization, stemming, and lemmatization.\"\n",
        "]\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform documents\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert to DataFrame for readability\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(tfidf_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9haHUHdB-EK-",
        "outputId": "29e9e698-cc2e-4683-ddc4-ec90c8fa8638"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        and  artificial  computers     helps     human  includes  \\\n",
            "0  0.000000    0.000000   0.316228  0.316228  0.316228  0.000000   \n",
            "1  0.306504    0.403016   0.000000  0.000000  0.000000  0.306504   \n",
            "2  0.306504    0.000000   0.000000  0.000000  0.000000  0.306504   \n",
            "\n",
            "   intelligence  language  learning  lemmatization   machine   natural  \\\n",
            "0      0.000000  0.632456  0.000000       0.000000  0.000000  0.316228   \n",
            "1      0.403016  0.000000  0.403016       0.000000  0.403016  0.000000   \n",
            "2      0.000000  0.000000  0.000000       0.403016  0.000000  0.000000   \n",
            "\n",
            "        nlp  preprocessing  processing  stemming      text  tokenization  \\\n",
            "0  0.000000       0.000000    0.316228  0.000000  0.000000      0.000000   \n",
            "1  0.403016       0.000000    0.000000  0.000000  0.000000      0.000000   \n",
            "2  0.000000       0.403016    0.000000  0.403016  0.403016      0.403016   \n",
            "\n",
            "   understand  \n",
            "0    0.316228  \n",
            "1    0.000000  \n",
            "2    0.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vUq59QMK-dvl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}